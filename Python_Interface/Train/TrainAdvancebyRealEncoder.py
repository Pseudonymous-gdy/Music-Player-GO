"""
TrainAdvancebyRealEncoder.py

Pretrain the full LinUCB+RNN recommender using *real* song embeddings
(e.g. FMA + librosa log-mel vectors stored in an NPZ file).

Pipeline:
  - Load song embeddings from NPZ (generated by SongEncoder.py).
  - Wrap them as MusicItem instances -> playlist.
  - Build a pool of virtual users via UserSimulator (each has its own
    evolving true preference vector beta_t).
  - Use Recommender (LinUCB+RNN) to interact with users:
      * selection(): choose item according to LinUCB+RNN score.
      * UserSimulator.step(): return reward + true beta_t.
      * Update:
          - LinUCB (A_i, b_i) by hand.
          - RNN via *two* losses:
                reward_loss = (r_hat - r_true)^2
                pref_loss   = ||normalize(beta_hat) - normalize(beta_true)||^2
                combined    = reward_loss + lambda_pref * pref_loss

At the end, we save:
  - LinUCB parameters (A_i, b_i) via Recommender.save_params()
  - RNN parameters (rnn_*)     via Recommender.rnn_model.save_model()

The on-device recommender can then use `initialization=False` to load
these pretrained parameters.
"""

from __future__ import annotations

import argparse
import random
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple, List, Dict

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# ----------------------------------------------------------------------
# Local imports (paths)
# ----------------------------------------------------------------------
CURRENT_DIR = Path(__file__).resolve().parent
PARENT_DIR = CURRENT_DIR.parent
if str(PARENT_DIR) not in sys.path:
    sys.path.append(str(PARENT_DIR))

from Recommender import Recommender, MusicItem
from UserSimulator import UserSimulator


# ----------------------------------------------------------------------
# Episode stats container
# ----------------------------------------------------------------------
@dataclass
class EpisodeStats:
    combined_loss: float
    reward_loss: float
    pref_loss: float
    reward: float
    steps: int

    def as_tuple(self) -> Tuple[float, float, float, float, int]:
        return (
            self.combined_loss,
            self.reward_loss,
            self.pref_loss,
            self.reward,
            self.steps,
        )


# ----------------------------------------------------------------------
# Utils
# ----------------------------------------------------------------------
def seed_everything(seed: Optional[int]) -> None:
    """Seed Python, NumPy, and torch for repeatable experiments."""
    if seed is None:
        return
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)


def load_playlist_from_npz(npz_path: Path) -> List[MusicItem]:
    """
    Load song embeddings from NPZ and wrap them as MusicItem objects.

    NPZ format:
      - key:   track_id (string; usually file stem like "000123")
      - value: embedding vector of shape (dim,)

    Returns
    -------
    playlist : List[MusicItem]
        Each MusicItem.features is a np.ndarray (float64) with shape (dim,).
    """
    if not npz_path.exists():
        raise FileNotFoundError(f"Embedding NPZ not found: {npz_path}")

    data = np.load(str(npz_path), allow_pickle=False)
    playlist: List[MusicItem] = []

    for key in sorted(data.files):
        vec = data[key]
        # Ensure 1D float64
        vec = np.asarray(vec, dtype=np.float64).reshape(-1)
        item = MusicItem(id=key, features=vec, name=f"Track_{key}")
        playlist.append(item)

    if not playlist:
        raise RuntimeError(f"No embeddings found in NPZ: {npz_path}")

    dim = playlist[0].features.shape[0]
    print(f"[TrainAdvanceReal] Loaded {len(playlist)} songs with dim={dim} from {npz_path}")
    return playlist


def update_linucb_params(
    recommender: Recommender,
    item: MusicItem,
    reward: float,
) -> None:
    """
    Update LinUCB parameters (A, b) for a single item.

    This mirrors Recommender.feedback's linear part but *does not* touch
    the RNN (we train the RNN manually in this script).

    A_i <- A_i + x x^T
    b_i <- b_i + r x
    """
    x_a = np.asarray(item.features, dtype=np.float64).reshape(-1)
    recommender._A[item.id] += np.outer(x_a, x_a)
    recommender._b[item.id] += reward * x_a
    recommender.last_selected_id = item.id


# ----------------------------------------------------------------------
# One episode with dual loss training
# ----------------------------------------------------------------------
def run_episode_with_dual_loss(
    recommender: Recommender,
    user_sim: UserSimulator,
    rnn_optimizer: torch.optim.Optimizer,
    steps: int,
    lambda_pref: float,
    grad_clip: float,
) -> EpisodeStats:
    """
    Simulate one episode (listening session) and train the RNN with
    *two* losses:

      reward_loss: MSE between predicted reward and true reward.
      pref_loss:   L2 between normalized beta_hat and beta_true.

    We:
      - keep LinUCB updates purely in numpy (A, b),
      - use PyTorch optimizer only for the RNN inside Recommender.
    """
    assert recommender.policy == "LinUCB+", "This training assumes LinUCB+ (with RNN)."
    if not hasattr(recommender, "rnn_model"):
        raise RuntimeError("Recommender.rnn_model not found. Use policy='LinUCB+'.")

    rnn = recommender.rnn_model

    # Reset RNN hidden state for this episode (single user session)
    rnn.h_t_1 = torch.zeros(rnn.hidden_size, dtype=torch.float32)

    total_combined = 0.0
    total_reward_loss = 0.0
    total_pref_loss = 0.0
    total_reward = 0.0
    total_steps = 0

    for _ in range(steps):
        # ----------------------------------------------
        # 1) Algorithm selects one item (top-1)
        # ----------------------------------------------
        item = recommender.selection(n=1)[0]

        # ----------------------------------------------
        # 2) Environment responds with reward + true beta_t
        # ----------------------------------------------
        reward_out = user_sim.step(item, return_info=True)
        # UserSimulator.step returns (reward, info)
        reward, info = reward_out  # unpack
        reward = float(reward)
        beta_true_np = np.asarray(info["beta_t"], dtype=np.float32).reshape(-1)

        # ----------------------------------------------
        # 3) Prepare features
        # ----------------------------------------------
        x_a = np.asarray(item.features, dtype=np.float32).reshape(-1)
        # Normalize to unit norm for stability (same as in UserSimulator)
        norm = np.linalg.norm(x_a)
        if norm > 1e-8:
            x_a = x_a / norm

        # Linear baseline: theta_a = A^{-1} b
        theta_a = np.linalg.solve(
            recommender._A[item.id],
            recommender._b[item.id],
        )
        base_pred_val = float(np.dot(theta_a.astype(np.float32), x_a))

        # ----------------------------------------------
        # 4) RNN forward + dual loss
        # ----------------------------------------------
        x_tensor = torch.from_numpy(x_a).float()  # (dim,)
        beta_true_t = torch.from_numpy(beta_true_np).float()  # (dim,)

        # Use the RNN's "internal" hidden state h_t_1
        h_prev = rnn.h_t_1
        if h_prev is None:
            h_prev = torch.zeros(rnn.hidden_size, dtype=torch.float32)

        rnn_optimizer.zero_grad()

        # Forward: h_t, beta_hat
        h_t, beta_hat = rnn.forward(x_a, h_prev)

        # Store new hidden state for next step (detach to cut graph)
        rnn.h_t_1 = h_t.detach()

        # Normalize both betas (directional alignment)
        beta_hat_norm = beta_hat / (beta_hat.norm() + 1e-8)
        beta_true_norm = beta_true_t / (beta_true_t.norm() + 1e-8)

        pref_loss = torch.mean((beta_hat_norm - beta_true_norm) ** 2)

        # Reward prediction: base linear part + RNN contribution
        base_pred_t = torch.tensor(base_pred_val, dtype=torch.float32)
        pred_reward_t = torch.dot(beta_hat, x_tensor) + base_pred_t

        reward_t = torch.tensor(reward, dtype=torch.float32)
        reward_loss = F.mse_loss(pred_reward_t, reward_t)

        combined_loss = reward_loss + lambda_pref * pref_loss

        combined_loss.backward()
        if grad_clip is not None and grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(rnn.parameters(), grad_clip)
        rnn_optimizer.step()

        # ----------------------------------------------
        # 5) Update LinUCB (A, b) with true reward
        # ----------------------------------------------
        update_linucb_params(recommender, item, reward)

        # ----------------------------------------------
        # 6) Accumulate stats
        # ----------------------------------------------
        total_combined += float(combined_loss.item())
        total_reward_loss += float(reward_loss.item())
        total_pref_loss += float(pref_loss.item())
        total_reward += reward
        total_steps += 1

    # Average per step
    if total_steps > 0:
        avg_combined = total_combined / total_steps
        avg_reward_loss = total_reward_loss / total_steps
        avg_pref_loss = total_pref_loss / total_steps
    else:
        avg_combined = avg_reward_loss = avg_pref_loss = 0.0

    return EpisodeStats(
        combined_loss=avg_combined,
        reward_loss=avg_reward_loss,
        pref_loss=avg_pref_loss,
        reward=total_reward,
        steps=total_steps,
    )


# ----------------------------------------------------------------------
# Argument parsing
# ----------------------------------------------------------------------
def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Pretrain LinUCB+RNN Recommender with REAL song embeddings (FMA+librosa)."
    )
    parser.add_argument(
        "--embedding-npz",
        type=str,
        default=str(CURRENT_DIR / "Data" / "fma_song_embeddings_128d.npz"),
        help="NPZ file containing song embeddings generated by SongEncoder.py",
    )
    parser.add_argument(
        "--episodes",
        type=int,
        default=500,
        help="Number of simulated listening sessions (episodes)",
    )
    parser.add_argument(
        "--steps-per-episode",
        type=int,
        default=50,
        help="Number of interaction steps per episode",
    )
    parser.add_argument(
        "--hidden-size",
        type=int,
        default=64,
        help="Hidden size of the RNN inside the Recommender",
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=1.0,
        help="Exploration parameter alpha for LinUCB",
    )
    parser.add_argument(
        "--l2",
        type=float,
        default=1.0,
        help="L2 regularization for LinUCB's A matrices (diagonal initialization)",
    )
    parser.add_argument(
        "--discount",
        type=float,
        default=0.7,
        help="Discount factor for re-recommending the same item in Recommender",
    )
    parser.add_argument(
        "--storage",
        type=str,
        default=str(CURRENT_DIR / "recommender_params_real_encoder.npz"),
        help="NPZ path used by Recommender to store A, b, and RNN weights",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=2025,
        help="Random seed for reproducibility",
    )
    parser.add_argument(
        "--num-users",
        type=int,
        default=200,
        help=(
            "Number of virtual users in the pool. "
            "Each UserSimulator has its own long-term preference; "
            "each episode randomly picks one user."
        ),
    )
    parser.add_argument(
        "--lambda-pref",
        type=float,
        default=1.0,
        help="Weight for preference alignment loss in combined loss",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-3,
        help="Learning rate for Adam optimizer on RNN parameters",
    )
    parser.add_argument(
        "--grad-clip",
        type=float,
        default=1.0,
        help="Gradient clipping max norm for RNN (0 or negative disables clipping)",
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help=(
            "If set, load existing A, b, and RNN parameters from --storage and continue "
            "training instead of reinitializing them."
        ),
    )
    return parser


# ----------------------------------------------------------------------
# Main entry
# ----------------------------------------------------------------------
def main():
    args = build_parser().parse_args()
    seed_everything(args.seed)

    # 1) Load playlist from real embeddings
    emb_path = Path(args.embedding_npz).resolve()
    playlist = load_playlist_from_npz(emb_path)
    dim = playlist[0].features.shape[0]

    # 2) Build virtual users (each in same feature space of dimension dim)
    users: List[UserSimulator] = [
        UserSimulator(dim=dim, seed=args.seed + i) for i in range(args.num_users)
    ]

    # 3) Prepare storage path
    storage_path = Path(args.storage).resolve()
    if not storage_path.parent.exists():
        storage_path.parent.mkdir(parents=True, exist_ok=True)

    # 4) Instantiate Recommender with LinUCB+ policy and internal RNN
    recommender = Recommender(
        storage=str(storage_path),
        playlist=playlist,
        alpha=args.alpha,
        l2=args.l2,
        initialization=not args.resume,  # if resume, try to load from storage
        policy="LinUCB+",
        discount=args.discount,
        hidden_size=args.hidden_size,  # forwarded via **kwargs to RNN
    )

    if not hasattr(recommender, "rnn_model"):
        raise RuntimeError("Recommender did not create rnn_model. Check policy='LinUCB+'.")

    rnn = recommender.rnn_model
    rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=args.lr)

    start = time.time()
    for episode in range(1, args.episodes + 1):
        # Pick which virtual user to use this episode (multi-user pool)
        user = random.choice(users)
        # Reset only short-term state (same user, new session)
        user.reset(resample_global=False)

        stats = run_episode_with_dual_loss(
            recommender=recommender,
            user_sim=user,
            rnn_optimizer=rnn_optimizer,
            steps=args.steps_per_episode,
            lambda_pref=args.lambda_pref,
            grad_clip=args.grad_clip,
        )

        avg_combined = stats.combined_loss
        avg_reward_loss = stats.reward_loss
        avg_pref_loss = stats.pref_loss
        avg_reward = stats.reward / max(stats.steps, 1)
        elapsed = time.time() - start

        print(
            f"Episode {episode:03d}/{args.episodes} | "
            f"avg_combined={avg_combined:.4f} | "
            f"avg_reward_loss={avg_reward_loss:.4f} | "
            f"avg_pref_loss={avg_pref_loss:.4f} | "
            f"avg_reward={avg_reward:.4f} | "
            f"elapsed={elapsed:.1f}s"
        )

    # 5) Save LinUCB and RNN parameters
    recommender.save_params()
    if hasattr(recommender, "rnn_model"):
        recommender.rnn_model.save_model()

    print(f"[TrainAdvanceReal] Saved pretrained Recommender (LinUCB+RNN) parameters to {storage_path}")


if __name__ == "__main__":
    main()
